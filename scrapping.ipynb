{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports necessary to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``tmp`` will be the destination directory for the downloaded files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "try:\n",
    "    if( not os.path.exists(cwd+'\\\\tmp')):\n",
    "        os.mkdir(cwd+'\\\\tmp')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important ``configs`` for the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "options.add_experimental_option(\"prefs\", {\n",
    "    'download.prompt_for_download':False,\n",
    "    'plugins.always_open_pdf_externally':True,\n",
    "    \"download.default_directory\": cwd+'\\\\tmp',\n",
    "    \"safebrowsing_for_trusted_sources_enabled\": False,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"safebrowsing.enabled\": True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = \"https://ocw.mit.edu/\" \n",
    "# base_url = \"https://ocw.mit.edu/courses/18-s191-introduction-to-computational-thinking-fall-2022/download/\" \n",
    "base_url = \"https://ocw.mit.edu/search/?q=database&t=Computer%20Science\" \n",
    "# base_url = \"https://ocw.mit.edu/search/?t=Computer%20Science\" \n",
    "\n",
    "browser = webdriver.Chrome(options=options)  # Optional argument, if not specified will search path.\n",
    "\n",
    "browser.get(base_url)\n",
    "browser.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Courses`` will be the directory to structure the scrapped contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if(not os.path.exists(cwd+'\\\\Courses')):\n",
    "        os.mkdir(cwd+'\\\\Courses')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``counter`` will be a pointer which show the current course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block will enter the first page of the course and will see if it has content to be downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickable = browser.find_element(By.ID, f'search-result-{counter}-title')\n",
    "clickable.click()\n",
    "browser.implicitly_wait(5)\n",
    "\n",
    "try:\n",
    "    click_download = browser.find_element(By.XPATH, '//html/body/div[1]/div[1]/div[2]/div[3]/div/div/div/div[2]/div/a')\n",
    "except NoSuchElementException as e:\n",
    "    print('Nothing to Download...')\n",
    "    counter+=1\n",
    "    browser.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "click_download.click()\n",
    "browser.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.page_source\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block will see the contents to be downloaded, if thats none, then go back to the initial page, if it has then lets start the digging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = soup.find_all(\"div\", class_= \"resource-list-toggle\")\n",
    "# print(resources)\n",
    "if len(resources)==0:\n",
    "    counter+=1\n",
    "    browser.execute_script(\"window.history.go(-1)\")\n",
    "    browser.execute_script(\"window.history.go(-1)\")\n",
    "    pass\n",
    "\n",
    "title:str = soup.find('h1').text\n",
    "title = title.strip('/\\n')\n",
    "title = title.replace(':','-')\n",
    "title = '_'.join(title.split(' '))\n",
    "\n",
    "try:\n",
    "    if(not os.path.exists(cwd+'\\\\Courses'+'\\\\'+title)):\n",
    "        os.mkdir(cwd+'\\\\Courses'+'\\\\'+title)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshot some complexity in the HTML Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = soup.find_all(\"div\", class_= \"resource-list\")\n",
    "indexes = []\n",
    "\n",
    "for i,x in enumerate(checker):\n",
    "    if x.find('h4'):\n",
    "        indexes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 6]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.page_source\n",
    "soup = BeautifulSoup(html)\n",
    "res = soup.find_all('a',class_='resource-thumbnail')\n",
    "# print(len(res))\n",
    "for x in res:\n",
    "    click_assignment_download_sell_all = browser.find_element(By.XPATH,f'//a[contains(@href,\"{x.attrs['href']}\")]')\n",
    "    click_assignment_download_sell_all.click()\n",
    "    time.sleep(0.5)\n",
    "has_see_all=True\n",
    "browser.implicitly_wait(2)\n",
    "browser.execute_script(\"window.history.go(-1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(resources):\n",
    "    \n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    section_name:str = x.find('h4').text\n",
    "   \n",
    "    # print(indexes[i]+1)\n",
    "    # print(f'//*[@id=\"main-course-section\"]/div/div[2]/div[{indexes[i]+1}]/div[1]/a')\n",
    "   \n",
    "    if len(resources) == 1:\n",
    "        click_lectures = browser.find_element(By.XPATH, f'//*[@id=\"main-course-section\"]/div/div[2]/div/div[1]/a')\n",
    "    else:\n",
    "        # print(f'//*[@id=\"main-course-section\"]/div/div[2]/div[{indexes[i]+1}]/div[1]/a')\n",
    "        click_lectures = browser.find_element(By.XPATH, f'//*[@id=\"main-course-section\"]/div/div[2]/div[{indexes[i]+1}]/div[1]/a')\n",
    "\n",
    "    click_lectures.click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    has_see_all=False\n",
    "    try:\n",
    "        click_see_all = browser.find_element(By.XPATH, f'/html/body/div[1]/div[2]/div[2]/div[2]/div/div/div/div[1]/div/div[2]/div[{indexes[i]+1}]/div[2]/div[11]/div/div/a')\n",
    "        click_see_all.click()\n",
    "        browser.implicitly_wait(3)\n",
    "        \n",
    "        \n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html)\n",
    "        res = soup.find_all('a',class_='resource-thumbnail')\n",
    "        # print(len(res))\n",
    "        for x in res:\n",
    "            click_assignment_download_sell_all = browser.find_element(By.XPATH,f'//a[contains(@href,\"{x.attrs['href']}\")]')\n",
    "            click_assignment_download_sell_all.click()\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            \n",
    "        has_see_all=True\n",
    "        browser.execute_script(\"window.history.go(-1)\")\n",
    "        browser.implicitly_wait(3)\n",
    "            \n",
    "    except NoSuchElementException as e:\n",
    "        print('No See All encountered!')\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if not has_see_all:\n",
    "        res = soup.find_all(id=re.compile(\"resource-list-container\"))\n",
    "\n",
    "        for x in res[i].find_all('a', href=True):\n",
    "            if re.search('\\..{1,4}$',x['href']):\n",
    "                # print(x)\n",
    "                # print(x['href'])\n",
    "                # leactures_pdf.append(x['href'])\n",
    "                click_assignment_download = browser.find_element(By.XPATH,f'//a[contains(@href,\"{x['href']}\")]')\n",
    "                # print(f'//a[contains(@href,\"{x['href']}\")]')\n",
    "                click_assignment_download.click()\n",
    "                time.sleep(0.5)\n",
    "        \n",
    "    list_of_files = os.listdir(os.getcwd() + '\\\\tmp')\n",
    "    section_name = section_name.replace(':','-')\n",
    "    section_name = '_'.join(section_name.split(' '))\n",
    "    \n",
    "    for file in list_of_files:\n",
    "        origin_path = os.getcwd() + '\\\\tmp'+'\\\\'+file\n",
    "        try:\n",
    "            os.mkdir(os.getcwd() + '\\\\Courses'+'\\\\'+ title+'\\\\'+ section_name)\n",
    "        except:\n",
    "            pass\n",
    "        destination_path =os.getcwd() + '\\\\Courses'+'\\\\'+title+'\\\\'+ section_name+'\\\\'+file\n",
    "        os.replace(origin_path,destination_path)\n",
    "        \n",
    "    \n",
    "    click_lectures.click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "counter+=1\n",
    "browser.execute_script(\"window.history.go(-1)\")\n",
    "browser.execute_script(\"window.history.go(-1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scroll_height = 70\n",
    "# browser.execute_script(f'window.scrollTo(0, {700+counter*scroll_height})')\n",
    "browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['search-result-10-title', 'search-result-10-title']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('search-result-10-title',browser.page_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickable = browser.find_element(By.ID, f'search-result-13-title')\n",
    "clickable.click()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
